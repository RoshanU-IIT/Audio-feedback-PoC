!pip install gradio transformers torch
import numpy as np
import pandas as pd
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
import gradio as gr
import warnings
warnings.filterwarnings('ignore')

class AudioFeedbackApp:
    def __init__(self):
        # Initialize the whisper model for transcription
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        
        # Load models
        self.model_id = "openai/whisper-base"
        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(
            self.model_id, torch_dtype=self.torch_dtype, low_cpu_mem_usage=True
        )
        self.processor = AutoProcessor.from_pretrained(self.model_id)
# Initialize sentiment analyzer
        self.sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english",
            device=self.device
        )

    def process_audio(self, audio_path):
        """Process audio file and return transcription and feedback"""
        try:
            # Create pipeline for audio transcription
            pipe = pipeline(
                "automatic-speech-recognition",
                model=self.model,
                tokenizer=self.processor.tokenizer,
                feature_extractor=self.processor.feature_extractor,
                max_new_tokens=128,
                torch_dtype=self.torch_dtype,
                device=self.device,
            )
            
            # Transcribe audio
            result = pipe(audio_path)
            transcript = result["text"]
            
            # Segment speakers
segments = self._segment_speakers(transcript)
            
            # Generate feedback
            feedback = self.generate_feedback(segments)
            
            return segments, feedback
            
        except Exception as e:
            return None, f"Error processing audio: {str(e)}"

    def _segment_speakers(self, transcript):
        """Segment transcript into different speakers"""
        sentences = [s.strip() for s in transcript.split('.') if s.strip()]
        segments = []
        current_speaker = 0
        
        for sentence in sentences:
            segments.append({
                'speaker': f'Speaker {current_speaker}',
                'text': sentence
            })
            current_speaker = 1 - current_speaker  # Alternate between speakers
return segments

    def generate_feedback(self, segments):
        """Generate feedback for each speaker"""
        feedback = {}
        
        for segment in segments:
            speaker = segment['speaker']
            text = segment['text']
            
            if speaker not in feedback:
                feedback[speaker] = {
                    'segments': [],
                    'metrics': {
                        'clarity': 0,
                        'engagement': 0,
                        'confidence': 0
                    }
                }
            
            # Analyze sentiment
            sentiment = self.sentiment_analyzer(text)[0]
            
            # Generate metrics
clarity = min(len(text.split()) / 20, 1.0)  # Based on sentence length
            engagement = 0.5 + (0.5 * (1 if sentiment['label'] == 'POSITIVE' else -1))
            confidence = sentiment['score']
            
            # Generate segment feedback
            segment_feedback = {
                'text': text,
                'clarity': clarity,
                'engagement': engagement,
                'confidence': confidence,
                'suggestions': self._generate_suggestions(clarity, engagement, confidence)
            }
            
            feedback[speaker]['segments'].append(segment_feedback)
            
            # Update overall metrics
            feedback[speaker]['metrics']['clarity'] += clarity
            feedback[speaker]['metrics']['engagement'] += engagement
            feedback[speaker]['metrics']['confidence'] += confidence
        
        # Calculate averages
        for speaker in feedback:
            num_segments = len(feedback[speaker]['segments'])
            for metric in feedback[speaker]['metrics']:
                feedback[speaker]['metrics'][metric] /= num_segments
return feedback

    def _generate_suggestions(self, clarity, engagement, confidence):
        """Generate specific suggestions based on metrics"""
        suggestions = []
        
        if clarity < 0.7:
            suggestions.append("Try using shorter, clearer sentences")
        if engagement < 0.6:
            suggestions.append("Consider using more engaging language and tone")
        if confidence < 0.6:
            suggestions.append("Work on speaking with more confidence")
            
        return suggestions

def create_interface():
    """Create Gradio interface"""
    def process_audio_file(audio, feedback_response=None):
        app = AudioFeedbackApp()
        segments, feedback = app.process_audio(audio)
        
        if segments is None:
            return f"Error: {feedback}"
# Format output
        output = "Transcription:\n\n"
        for segment in segments:
            output += f"{segment['speaker']}: {segment['text']}\n"
            
        output += "\nFeedback:\n"
        for speaker, data in feedback.items():
            output += f"\n{speaker}:\n"
            output += f"Overall Metrics:\n"
            for metric, value in data['metrics'].items():
                output += f"- {metric}: {value:.2f}\n"
                
            output += "\nDetailed Feedback:\n"
            for segment in data['segments']:
                output += f"Text: {segment['text']}\n"
                output += "Suggestions:\n"
                for suggestion in segment['suggestions']:
                    output += f"- {suggestion}\n"
                output += "---\n"
        
        return output

    # Create Gradio interface
iface = gr.Interface(
        fn=process_audio_file,
        inputs=[
            gr.Audio(type="filepath", label="Upload Audio File"),
            gr.Textbox(label="Feedback Response (Optional)")
        ],
        outputs=gr.Textbox(label="Results"),
        title="Audio Conversation Feedback Tool",
        description="Upload an audio file to receive speaker-labeled transcription and feedback."
    )
    
    return iface

# Run the application
if __name__ == "__main__":
    iface = create_interface()
    iface.launch(share=True)
